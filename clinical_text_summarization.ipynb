{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ***CLINICAL TEXT SUMMARIZATION LAB***"
      ],
      "metadata": {
        "id": "wdHByto50VS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "eOYit-HH6trT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the most important libraries we'll be using for this lab:\n",
        "\n",
        "\n",
        "\n",
        "*   **SpaCy** - Efficient text processing\n",
        "*   **Tika** - Extract PDF data\n",
        "*   **Rouge_Score** - Model performance metric\n",
        "*   **en_core_web_lg** - Small English Language model with \"tokenization\" and \"lemmatization\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-_8VnZpO9quj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "%pip install spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "%pip install tika\n",
        "%pip install rouge_score"
      ],
      "metadata": {
        "id": "5UIMG2nj5ubY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Libraries\n",
        "import spacy\n",
        "import re\n",
        "from tika import parser\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "BAGdg2uH7n7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this next cell, we will be retrieving a key from ML platform **HuggingFace**."
      ],
      "metadata": {
        "id": "sqMbf4MqfVik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "userdata.get('NDL') # Grab key"
      ],
      "metadata": {
        "id": "mNoOZfvw6THw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing & Extraction"
      ],
      "metadata": {
        "id": "IQihH14p63ZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize clinical documents, we first need the raw text. Here, we define a `parser` object from  the `tika` library to extract raw text from a PDF file."
      ],
      "metadata": {
        "id": "zv3tmJmIgEm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "raw_text = parser.from_file(\"ENGERIX-B.pdf\")"
      ],
      "metadata": {
        "id": "k7v34V-y6doR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: `raw_text` is a dictionary. To access the words, we call `raw_text['content']`\n",
        "\n"
      ],
      "metadata": {
        "id": "hM9fJ8A4hBrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_text = raw_text['content']\n",
        "content_text"
      ],
      "metadata": {
        "id": "oGDsO2w4ENFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These next few steps are important. We need to **clean** our document to avoid any errors. Specifically, we don't want our model to think `'  '` is a valid character, or a space in between paragraphs."
      ],
      "metadata": {
        "id": "niN0w0UAjj7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  \"\"\"Cleans extra lines and extra whitespace.\"\"\"\n",
        "  text = re.sub(r'\\n+', ' ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  return text\n",
        "\n",
        "cleaned_text = clean_text(content_text)"
      ],
      "metadata": {
        "id": "hERVBT756i36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we pass our document to the `en_core_web_lg` model for auto **tokenization** and **lemmatization**."
      ],
      "metadata": {
        "id": "J8jhZv3tkbbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\") # Model w/ auto tokenization and lemmatization\n",
        "doc = nlp(cleaned_text)\n",
        "\n",
        "doc"
      ],
      "metadata": {
        "id": "rOPIW90iBIpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it ðŸ˜Ž"
      ],
      "metadata": {
        "id": "V9Pv3V5PkmMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "for sent in doc.sents:\n",
        "  cleaned_sentence = sent.text.strip()\n",
        "  if cleaned_sentence:\n",
        "    sentences.append(cleaned_sentence)"
      ],
      "metadata": {
        "id": "7EeqjPpvGTqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization and Lemmatization have very subtle differences. Let's look at them :)"
      ],
      "metadata": {
        "id": "CrPmGOShnnXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of what tokenization looks like vs Lemmatization\n",
        "tokens = []\n",
        "lemmas = []\n",
        "\n",
        "for token in doc:\n",
        "  tokens.append(token.text)\n",
        "  lemmas.append(token.lemma_)\n",
        "\n",
        "print(\"First 10 tokens:\", tokens[:10])\n",
        "print(\"First 10 lemmas:\", lemmas[:10])"
      ],
      "metadata": {
        "id": "NN2F3IzsFSaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarization Model\n",
        "\n",
        "The moment you've ALL been waiting for. Our text is loaded in and preprocessed, and now, we pass the text into a **pre-trained** model, `t5-large`, to do the summarization for us."
      ],
      "metadata": {
        "id": "n_fGvRYRO6GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"t5-large\"\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model_name) # Load model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name) # Count tokens accurately"
      ],
      "metadata": {
        "id": "A_K16xhi6-dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text_by_sentence(sentences, max_tokens=512):\n",
        "  \"\"\"\n",
        "  Splits a list of sentences into chunks, ensuring each chunk doesn't exceed the word limit.\n",
        "  For large texts, this step ensures we don't exceed our computer's memory limits.\n",
        "  \"\"\"\n",
        "\n",
        "  chunks, current_chunk = [], []\n",
        "  current_tokens = 0\n",
        "\n",
        "  for sentence in sentences:\n",
        "    sentence_tokens = len(tokenizer.encode(sentence, add_special_tokens=False))\n",
        "\n",
        "    # If adding this sentence exceeds max_tokens, save the current chunk and start a new one\n",
        "    if current_tokens + sentence_tokens > max_tokens:\n",
        "      if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "      current_chunk, current_tokens = [sentence], sentence_tokens\n",
        "    else:\n",
        "      current_chunk.append(sentence)\n",
        "      current_tokens += sentence_tokens\n",
        "\n",
        "    # Add the last chunk if it contains any sentences\n",
        "  if current_chunk:\n",
        "    chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "QS6gLE-F7Cek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = chunk_text_by_sentence(sentences, max_tokens=300)\n",
        "\n",
        "len(text_chunks)"
      ],
      "metadata": {
        "id": "t7xLh3z1RqmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def abstractive_summary(chunks):\n",
        "  \"\"\"\n",
        "  Summarizes each chunk individually, then merges them into one.\n",
        "  \"\"\"\n",
        "\n",
        "  chunk_summaries = []\n",
        "  for chunk in chunks:\n",
        "    # We are summarizing each chunk\n",
        "    summary = summarizer(chunk, max_length=50, min_length=20, do_sample=False)[0]['summary_text']\n",
        "    chunk_summaries.append(summary)\n",
        "\n",
        "  # we are just combining all the chunk summaries into 1\n",
        "  combined_summary = \" \".join(chunk_summaries)\n",
        "\n",
        "  # this is the final summary\n",
        "  final_summary = summarizer(combined_summary, max_length=100, min_length=50, do_sample=False)[0]['summary_text']\n",
        "\n",
        "  return final_summary"
      ],
      "metadata": {
        "id": "sc7eSTdE7HGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "final_summary = abstractive_summary(text_chunks)"
      ],
      "metadata": {
        "id": "Uzr3kbIKSIFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning the summary further. Talk about how this really depends dataset to dataset\n",
        "def clean_summary(summary):\n",
        "  summary = re.sub(r'\\b(glaxosmithkline|vaers|wwwvaershhsgov|dosage and administration|contact.*?vaers)\\b', '', summary, flags=re.IGNORECASE)\n",
        "  summary = re.sub(r'\\s+', ' ', summary).strip()\n",
        "  return summary\n",
        "\n",
        "final_summary = clean_summary(final_summary)\n",
        "print(\"\\nFinal Concise Summary:\", final_summary)"
      ],
      "metadata": {
        "id": "F4HWa_FK7KT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "zaatFaxZQB4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference summary for ROUGE evaluation\n",
        "reference_summary = \"\"\"\n",
        "ENGERIX-B is a vaccine indicated for immunization against infection caused by all known subtypes of hepatitis B virus.\n",
        "It is administered intramuscularly in a three-dose schedule for most individuals and a four-dose schedule for adults on hemodialysis.\n",
        "Available as a sterile suspension in prefilled syringes and vials.\n",
        "Contraindicated in individuals with severe allergic reactions to any hepatitis B vaccine component.\n",
        "Common adverse reactions include injection-site soreness (22%) and fatigue (14%).\n",
        "Fainting can occur after administration, and precautions should be taken.\n",
        "Infants with low birth weight and premature infants require special considerations.\n",
        "Should not be mixed with other vaccines in the same syringe.\n",
        "Lower antibody responses observed in individuals over 60 years of age.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FGLeiMV17aKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(reference_summary, final_summary)\n",
        "print(\"\\nROUGE scores:\", scores)"
      ],
      "metadata": {
        "id": "ekaXFrts7dEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing ROUGE scores using Plotly\n",
        "df = pd.DataFrame({\n",
        "    \"Metric\": [\"Precision\", \"Recall\", \"F1-Score\"],\n",
        "    \"ROUGE-1\": [scores[\"rouge1\"].precision, scores[\"rouge1\"].recall, scores[\"rouge1\"].fmeasure],\n",
        "    \"ROUGE-2\": [scores[\"rouge2\"].precision, scores[\"rouge2\"].recall, scores[\"rouge2\"].fmeasure],\n",
        "    \"ROUGE-L\": [scores[\"rougeL\"].precision, scores[\"rougeL\"].recall, scores[\"rougeL\"].fmeasure],\n",
        "})\n",
        "\n",
        "fig = px.bar(df, x=\"Metric\", y=[\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"],\n",
        "             barmode=\"group\", title=\"ROUGE Score Comparison\",\n",
        "             labels={\"value\": \"Score\", \"variable\": \"ROUGE Metric\"})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "C067OMsu7RvH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}